{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you begin, open this experiment on Trovi:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-   Use this link: [Large-scale model training on Chameleon](https://trovi.chameleoncloud.org/dashboard/artifacts/bd06bd6d-d94f-4297-ad5d-c9b7e1f02575) on Trovi\n",
    "-   Then, click “Launch on Chameleon”. This will start a new Jupyter server for you, with the experiment materials already in it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inside the `llm-chi` directory, open the `multi` subdirectory. You will see several notebooks - look for the one titled `2_create_server.ipynb`. Open this notebook and continue there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bring up a GPU server\n",
    "\n",
    "At the beginning of the lease time, we will bring up our GPU server. We will use the `python-chi` Python API to Chameleon to provision our server.\n",
    "\n",
    "We will execute the cells in this notebook inside the Chameleon Jupyter environment.\n",
    "\n",
    "Run the following cell, and make sure the correct project is selected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "from chi import server, context, lease, network\n",
    "import chi, os, time\n",
    "\n",
    "context.version = \"1.0\"\n",
    "context.choose_project()\n",
    "context.choose_site(default=\"KVM@TACC\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the string in the following cell to reflect the name of *your* lease (**with your own net ID**), then run it to get your lease:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "l = lease.get_lease(f\"llm_multi_netID\")\n",
    "l.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The status should show as “ACTIVE” now that we are past the lease start time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of this notebook sets up the instance and the experiment environment, which cumulatively takes a while - bringing up the instance can take some time, and building the container image can take some time.\n",
    "\n",
    "But, you can be mostly hands-off in this stage. You can save time by clicking on this cell, then selecting Run \\> Run Selected Cell and All Below from the Jupyter menu.\n",
    "\n",
    "As the notebook executes, monitor its progress to make sure it does not get stuck on any execution error, and also to see what it is doing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the lease to bring up a server with the `CC-Ubuntu24.04-CUDA` disk image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default boot disk for instances at KVM@TACC is a little small for large model training, so we will first create a larger boot volume (200 GiB) from that image, then boot the server from that volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "username = os.getenv('USER') # all exp resources will have this suffix\n",
    "server_name = f\"node-llm-multi-{username}\"\n",
    "\n",
    "try:\n",
    "    s = server.get_server(server_name)\n",
    "    print(f\"Server {server_name} already exists. Skipping create.\")\n",
    "except Exception:\n",
    "    os_conn = chi.clients.connection()\n",
    "    cinder_client = chi.clients.cinder()\n",
    "\n",
    "    images = list(os_conn.image.images(name=\"CC-Ubuntu24.04-CUDA\"))\n",
    "    image_id = images[0].id\n",
    "\n",
    "    boot_vol = cinder_client.volumes.create(\n",
    "        name=f\"boot-vol-llm-multi-{username}\",\n",
    "        size=200,\n",
    "        imageRef=image_id,\n",
    "    )\n",
    "\n",
    "    while True:\n",
    "        boot_vol = cinder_client.volumes.get(boot_vol.id)\n",
    "        if boot_vol.status == \"available\":\n",
    "            break\n",
    "        if boot_vol.status in [\"error\", \"error_restoring\", \"error_extending\"]:\n",
    "            raise RuntimeError(f\"Boot volume provisioning failed with status {boot_vol.status}\")\n",
    "        time.sleep(10)\n",
    "\n",
    "    bdm = [{\n",
    "        \"boot_index\": 0,\n",
    "        \"uuid\": boot_vol.id,\n",
    "        \"source_type\": \"volume\",\n",
    "        \"destination_type\": \"volume\",\n",
    "        \"delete_on_termination\": True,\n",
    "    }]\n",
    "\n",
    "    server_from_vol = os_conn.compute.create_server(\n",
    "        name=server_name,\n",
    "        flavor_id=server.get_flavor_id(l.get_reserved_flavors()[0].name),\n",
    "        block_device_mapping_v2=bdm,\n",
    "        networks=[{\"uuid\": os_conn.network.find_network(\"sharednet1\").id}],\n",
    "    )\n",
    "\n",
    "    os_conn.compute.wait_for_server(server_from_vol)\n",
    "    s = server.get_server(server_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need security groups to allow SSH and Jupyter access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "security_groups = [\n",
    "  {'name': \"allow-ssh\", 'port': 22, 'description': \"Enable SSH traffic on TCP port 22\"},\n",
    "  {'name': \"allow-8888\", 'port': 8888, 'description': \"Enable TCP port 8888 (used by Jupyter)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "for sg in security_groups:\n",
    "  secgroup = network.SecurityGroup({\n",
    "      'name': sg['name'],\n",
    "      'description': sg['description'],\n",
    "  })\n",
    "  secgroup.add_rule(direction='ingress', protocol='tcp', port=sg['port'])\n",
    "  secgroup.submit(idempotent=True)\n",
    "  s.add_security_group(sg['name'])\n",
    "\n",
    "print(f\"updated security groups: {[sg['name'] for sg in security_groups]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we’ll associate a floating IP with the instance, so that we can access it over SSH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.associate_floating_ip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.refresh()\n",
    "s.check_connectivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output below, make a note of the floating IP that has been assigned to your instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.refresh()\n",
    "s.show(type=\"widget\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve code and notebooks on the instance\n",
    "\n",
    "Now, we can use `python-chi` to execute commands on the instance, to set it up. We’ll start by retrieving the code and other materials on the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.execute(\"git clone --branch h100 --single-branch https://github.com/teaching-on-testbeds/llm-chi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Docker with NVIDIA container toolkit\n",
    "\n",
    "To use common deep learning frameworks like Tensorflow or PyTorch, we can run containers that have all the prerequisite libraries necessary for these frameworks. Here, we will set up the container framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.execute(\"curl -sSL https://get.docker.com/ | sudo sh\")\n",
    "s.execute(\"sudo groupadd -f docker; sudo usermod -aG docker $USER\")\n",
    "s.execute(\"docker run hello-world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also install the NVIDIA container toolkit, with which we can access GPUs from inside our containers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "# get NVIDIA container toolkit \n",
    "s.execute(\"curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n",
    "  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n",
    "    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n",
    "    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\")\n",
    "s.execute(\"sudo apt update\")\n",
    "s.execute(\"sudo apt-get install -y nvidia-container-toolkit\")\n",
    "s.execute(\"sudo nvidia-ctk runtime configure --runtime=docker\")\n",
    "# for https://github.com/NVIDIA/nvidia-container-toolkit/issues/48\n",
    "s.execute(\"sudo jq 'if has(\\\"exec-opts\\\") then . else . + {\\\"exec-opts\\\": [\\\"native.cgroupdriver=cgroupfs\\\"]} end' /etc/docker/daemon.json | sudo tee /etc/docker/daemon.json.tmp > /dev/null && sudo mv /etc/docker/daemon.json.tmp /etc/docker/daemon.json\")\n",
    "s.execute(\"sudo systemctl restart docker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we will verify that we can see our NVIDIA GPUs from inside a container, by passing `--gpus all`. (The `-rm` flag says to clean up the container and remove its filesystem when it finishes running.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.execute(\"docker run --rm --gpus all ubuntu nvidia-smi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and start container for “Multiple GPU” section\n",
    "\n",
    "Let’s build the container image that we are going to use for this lab from `multi/docker/Dockerfile`. It may take 10-15 minutes to build this container image.\n",
    "\n",
    "You may view this Dockerfile in our Github repository: [multi/docker/Dockerfile](https://github.com/teaching-on-testbeds/llm-chi/blob/main/multi/docker/Dockerfile).\n",
    "\n",
    "This image starts from the Jupyter Pytorch CUDA12 stack and adds the pieces we need for this lab:\n",
    "\n",
    "-   `nvtop` for NVIDIA GPU monitoring\n",
    "-   ML Python libraries: notably, Lightning, Transformers and related libraries, and BitsAndBytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.execute(\"docker build -t llm-jupyter:latest ~/llm-chi/multi/docker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and get it running:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.execute(\"docker run --rm -d -p 8888:8888 -v /home/cc/llm-chi/multi/workspace:/home/jovyan/work --gpus all --name jupyter llm-jupyter:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the Jupyter service, we will need its randomly generated secret token (which secures it from unauthorized access). We’ll get this token by running `jupyter server list` inside the `jupyter` container on the `node-llm-<username>` instance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run in Chameleon Jupyter environment\n",
    "s.execute(\"docker exec jupyter jupyter server list\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for a line like\n",
    "\n",
    "    http://localhost:8888/lab?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "\n",
    "Paste this into a browser tab, but in place of `localhost`, substitute the floating IP assigned to your instance, to open the Jupyter notebook interface.\n",
    "\n",
    "Continue with the notebook located inside that workspace."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": "3"
   },
   "file_extension": ".py",
   "mimetype": "text/x-python"
  }
 }
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a large model on a single GPU\n",
    "\n",
    "In this section, we will practice strategies for training a large model on a single GPU. After completing this section, you should understand the effect of\n",
    "\n",
    "-   batch size\n",
    "-   gradient accumulation\n",
    "-   reduced precision/mixed precision\n",
    "-   CPU offload\n",
    "-   activation checkpointing\n",
    "-   parameter efficient fine tuning\n",
    "\n",
    "on a large model training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that you can see the GPU inside the container:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Throughout these experiments, we will monitor GPU compute and memory utilization with `nvtop`. Open a terminal in the Jupyter service on node-llm-single (File \\> New \\> Terminal) and in it, run\n",
    "\n",
    "``` bash\n",
    "# runs in the Jupyter service on node-llm-single\n",
    "nvtop\n",
    "```\n",
    "\n",
    "In this display,\n",
    "\n",
    "-   `GPU%` tells us how busy the GPU compute cores are. Low values with long training time often mean the GPU is waiting (for data, CPU work, or synchronization).\n",
    "-   `GPU mem%` tells us how much GPU VRAM is currently in use. If this approaches 100%, we are close to OOM.\n",
    "\n",
    "We will refer back to this display throughout the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the training scripts, download and unpack the dataset snapshot that we will use in this lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "cd ~/work\n",
    "mkdir -p data\n",
    "wget -O data/gourmetgram_caption.tar.gz \"https://nyu.box.com/shared/static/g3qw3g5j7l8dkvyf02a9afuuo9grs3g3.gz\"\n",
    "mkdir -p data/gourmetgram_caption\n",
    "tar -xzf data/gourmetgram_caption.tar.gz -C data/gourmetgram_caption --strip-components=1\n",
    "\n",
    "ls -lah data/gourmetgram_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training scripts now read the dataset from `./data/gourmetgram_caption`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Lightning workflow\n",
    "\n",
    "In this section, we will run a BLIP-2 fine-tuning script (`fine-tune-blip.py`) that is built on [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/).\n",
    "\n",
    "PyTorch Lightning helps us keep the training loop logic mostly fixed while we change strategy using config values like:\n",
    "\n",
    "-   batch size\n",
    "-   gradient accumulation\n",
    "-   precision mode (`32-true`, `bf16-mixed`, `bf16-true`)\n",
    "-   activation checkpointing\n",
    "-   optimizer choice\n",
    "-   LoRA/QLoRA options\n",
    "\n",
    "This means we can compare memory and training time across many settings by editing one config dictionary in a script, instead of rewriting training code each time.\n",
    "\n",
    "Our focus will be on comparing time and memory requirements under different settings - we aren’t trying to optimize for model quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s briefly review the training script, which is written for Pytorch Lightning. Lightning is Pytorch with less boilerplate and some additional functionality baked in, including things like distributed training.\n",
    "\n",
    "A basic Lightning module for our image captioning model would look something like this:\n",
    "\n",
    "``` python\n",
    "# Model\n",
    "class BLIP2FoodFineTuner(L.LightningModule):\n",
    "    def __init__(self, model_name, lr):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.processor = Blip2Processor.from_pretrained(model_name, use_fast=True)\n",
    "        self.model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            model_name,\n",
    "            dtype=torch.float32 \n",
    "        )\n",
    "        self.model.train()\n",
    "        self.lr = lr\n",
    "\n",
    "    def training_step(self, batch, _):\n",
    "        vision_inputs = self.processor(images=batch[\"image\"], return_tensors=\"pt\").to(self.device)\n",
    "        text_inputs = self.processor.tokenizer(\n",
    "            batch[\"text\"], padding=True, truncation=True, max_length=50, return_tensors=\"pt\"\n",
    "        ).to(self.device)\n",
    "\n",
    "        outputs = self.model(\n",
    "            input_ids=text_inputs[\"input_ids\"],\n",
    "            attention_mask=text_inputs[\"attention_mask\"],\n",
    "            pixel_values=vision_inputs[\"pixel_values\"],\n",
    "            labels=text_inputs[\"input_ids\"],\n",
    "        )\n",
    "\n",
    "        loss = outputs.loss\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "```\n",
    "\n",
    "But ours is a little more complicated for two reasons: configuration and logging.\n",
    "\n",
    "At the top, the `cfg = {...}` dictionary controls the experiment settings. Then, those settings are used by the Lightning `Trainer`. Lightning implements the techniques we learned about so we just have to specify them - we don’t have to implement them from scratch in Pytorch.\n",
    "\n",
    "``` python\n",
    "trainer = L.Trainer(\n",
    "    accelerator=\"gpu\",\n",
    "    devices=cfg[\"devices\"],\n",
    "    strategy=cfg[\"strategy\"],\n",
    "    precision=cfg[\"precision\"],\n",
    "    accumulate_grad_batches=cfg[\"accumulate_grad_batches\"],\n",
    "    max_epochs=cfg[\"max_epochs\"],\n",
    "    max_steps=cfg[\"max_steps\"],\n",
    "    limit_train_batches=cfg[\"limit_train_batches\"],\n",
    "    enable_checkpointing=cfg[\"enable_checkpointing\"],\n",
    "    enable_progress_bar=False,\n",
    "    log_every_n_steps=1\n",
    ")\n",
    "trainer.fit(model, train_loader)\n",
    "```\n",
    "\n",
    "We also use some of the configuration inside the Lightning module itself, e.g. \n",
    "\n",
    "``` python\n",
    "def configure_optimizers(self):\n",
    "    if cfg[\"optim\"] == \"adamw\":\n",
    "        return torch.optim.AdamW(self.model.parameters(), lr=self.lr)\n",
    "    if cfg[\"optim\"] == \"sgd\":\n",
    "        return torch.optim.SGD(self.model.parameters(), lr=self.lr)\n",
    "    if cfg[\"optim\"] == \"adam_8bit\":\n",
    "        return bnb.optim.Adam8bit(self.model.parameters(), lr=self.lr)\n",
    "    if cfg[\"optim\"] == \"deepspeed_cpu\":\n",
    "        return DeepSpeedCPUAdam(self.model.parameters(), lr=self.lr)\n",
    "```\n",
    "\n",
    "We have also added some extra logging, so that the script prints memory snapshots at key points in a training step:\n",
    "\n",
    "-   `_phase0_before_fwd`: right before the forward pass\n",
    "-   `_phase1_after_fwd`: after forward, before backward\n",
    "-   `_phase2_after_bwd`: after backward\n",
    "-   `_phase3_after_opt_step`: after optimizer step (when optimizer state is materialized)\n",
    "\n",
    "Lightning makes it easy for us to define custom code that runs at various points in the forward pass, backward pass, or optimizer step.\n",
    "\n",
    "Each memory snapshot will include:\n",
    "\n",
    "-   `Params`: model weights in memory\n",
    "-   `Grads`: gradient tensors\n",
    "-   `Optim`: optimizer state tensors\n",
    "-   `Acts`: activation memory (Note: we actually don’t have a good way to estimate this exactly, so we are assuming “whatever is left” is activation memory usage. In fact, this number as reported also includes other small non-activation items stored in memory, which are cumulatively less than 1GB.)\n",
    "-   `Allocated/peak`: current and max allocated CUDA memory seen so far\n",
    "-   `Reserved`: memory held by the CUDA allocator cache (can stay high even after tensors are freed)\n",
    "\n",
    "and it will print these in step 0, step 1, and then every 50 steps after that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open `fine-tune-blip.py` in the Jupyter file browser. We will use a baseline config first, then edit only a few keys at each experiment stage.\n",
    "\n",
    "For every experiment in this notebook, we will follow the same run loop:\n",
    "\n",
    "1.  edit the requested keys in `cfg`\n",
    "2.  run `python fine-tune-blip.py` in a terminal cell\n",
    "3.  record whether it succeeds or fails, plus run time and memory output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For quick reference, this table summarizes the configuration used in each full fine-tuning experiment.\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>\n",
    "Experiment\n",
    "</th>\n",
    "<th>\n",
    "Model\n",
    "</th>\n",
    "<th>\n",
    "bs/acc\n",
    "</th>\n",
    "<th>\n",
    "precision\n",
    "</th>\n",
    "<th>\n",
    "optim\n",
    "</th>\n",
    "<th>\n",
    "act_ckpt\n",
    "</th>\n",
    "<th>\n",
    "strategy\n",
    "</th>\n",
    "<th>\n",
    "max_steps\n",
    "</th>\n",
    "<th>\n",
    "Notes\n",
    "</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>\n",
    "Baseline\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-opt-2.7b</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>64/1</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>32-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adamw</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>lr=5e-6</code>, <code>num_train_samples=512</code>, OOM\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Reduced batch size\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-opt-2.7b</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>16/1</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>32-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adamw</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Gradient accumulation\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-opt-2.7b</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>16/4</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>32-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adamw</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Grad accum + LR x4 rerun\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-opt-2.7b</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>16/4</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>32-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adamw</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>lr=2e-5</code>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Reduced precision\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-opt-2.7b</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>16/4</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>bf16-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adamw</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Mixed precision\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-opt-2.7b</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>16/4</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>bf16-mixed</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adamw</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Larger model\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-opt-6.7b</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>32/2</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>bf16-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adamw</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Even larger model\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-flan-t5-xxl</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>32/2</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>bf16-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adamw</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "OOM\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "XXL + smallest batch\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-flan-t5-xxl</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>1/1</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>bf16-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adamw</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "OOM\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Optimizer without state\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-flan-t5-xxl</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>32/2</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>bf16-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>sgd</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "8-bit optimizer\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-flan-t5-xxl</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>2/2</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>bf16-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adam_8bit</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "Activation checkpointing\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-flan-t5-xxl</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>2/2</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>bf16-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adam_8bit</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>True</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "CPU offload (DeepSpeed)\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-flan-t5-xxl</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>32/2</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>bf16-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>deepspeed_cpu</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>deepspeed_stage_2_offload</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>2</code>\n",
    "</td>\n",
    "<td>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "And this table summarizes the PEFT experiments.\n",
    "\n",
    "<table>\n",
    "<thead>\n",
    "<tr>\n",
    "<th>\n",
    "Experiment\n",
    "</th>\n",
    "<th>\n",
    "Model\n",
    "</th>\n",
    "<th>\n",
    "bs/acc\n",
    "</th>\n",
    "<th>\n",
    "precision\n",
    "</th>\n",
    "<th>\n",
    "optim\n",
    "</th>\n",
    "<th>\n",
    "act_ckpt\n",
    "</th>\n",
    "<th>\n",
    "strategy\n",
    "</th>\n",
    "<th>\n",
    "max_steps\n",
    "</th>\n",
    "<th>\n",
    "Notes\n",
    "</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody>\n",
    "<tr>\n",
    "<td>\n",
    "LoRA\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-flan-t5-xxl</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>32/2</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>32-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adamw</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>lr=5e-6</code>, <code>num_train_samples=512</code>, <code>use_lora=True</code>, <code>use_qlora=False</code>\n",
    "</td>\n",
    "</tr>\n",
    "<tr>\n",
    "<td>\n",
    "QLoRA\n",
    "</td>\n",
    "<td>\n",
    "<code>blip2-flan-t5-xxl</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>64/1</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>32-true</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>adamw</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>False</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>auto</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>-1</code>\n",
    "</td>\n",
    "<td>\n",
    "<code>lr=5e-6</code>, <code>num_train_samples=512</code>, <code>use_lora=False</code>, <code>use_qlora=True</code>\n",
    "</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Baseline\n",
    "\n",
    "As a baseline, let’s try two epochs of fine-tuning Blip-2 using OPT-2.7b, an LLM with 2.7 billion parameters. When using Blip-2 with OPT-2.7b, the combined model will have 3.7 billion parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `cfg` in `fine-tune-blip.py` to this baseline:\n",
    "\n",
    "``` python\n",
    "cfg = {\n",
    "    \"model_name\": \"Salesforce/blip2-opt-2.7b\",\n",
    "    \"lr\": 5e-6,\n",
    "    \"batch_size\": 64,\n",
    "    \"accumulate_grad_batches\": 1,\n",
    "    \"precision\": \"32-true\",\n",
    "    \"optim\": \"adamw\",\n",
    "    \"act_ckpt\": False,\n",
    "    \"max_epochs\": 2,\n",
    "    \"max_steps\": -1,\n",
    "    \"devices\": 1,\n",
    "    \"strategy\": \"auto\",\n",
    "    \"num_workers\": 8,\n",
    "    \"limit_train_batches\": 1.0,\n",
    "    \"enable_checkpointing\": False,\n",
    "    \"num_train_samples\": 512,\n",
    "    \"save_model\": False,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run is expected to fail with OOM.\n",
    "\n",
    "In the logs, we should see that parameters alone take close to 14 GB at full precision (32-bit). Then,\n",
    "\n",
    "-   After the forward pass, activation memory spikes.\n",
    "-   After backward pass, activation memory is freed, but now gradients are saved in memory.\n",
    "-   After the optimizer step, optimizer state is allocated, memory jumps again, and the run goes OOM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Reduced batch size\n",
    "\n",
    "What if we reduce the batch size?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cfg`, change:\n",
    "\n",
    "-   `\"batch_size\": 64` -\\> `\"batch_size\": 16`\n",
    "\n",
    "Leave all other values the same as the baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run should now fit in memory, because with a smaller batch size, we need substantially less memory for activations.\n",
    "\n",
    "Make a note of the training time and memory, which is printed at the end of the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Gradient accumulation\n",
    "\n",
    "By using gradient accumulation to “step” only after a few “micro batches”, we can train with a larger effective “global” batch size, with minimal effect on the memory required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cfg`, change:\n",
    "\n",
    "-   `\"accumulate_grad_batches\": 1` -\\> `\"accumulate_grad_batches\": 4`\n",
    "\n",
    "Keep `batch_size` at `16` and leave all other values the same as the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With gradient accumulation, we will see more output lines per step, simply because there are multiple forward passes and backward passes before the optimizer step.\n",
    "\n",
    "Make a note of the training time and memory. We should see memory similar to the previous run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that the loss after two epochs is different from the previous run - since the effective batch size has changed, we should scale the learning rate by the same amount to have a similar total step size over an epoch.\n",
    "\n",
    "In the `cfg`, increase the learning rate by 4x (`\"lr\": 5e-6` -\\> `\"lr\": 2e-5`), and run again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Reduced precision\n",
    "\n",
    "With a “bf16” format for numbers, instead of “float32”, we can further reduce the memory required, although this representation is less precise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cfg`, change:\n",
    "\n",
    "-   `\"precision\": \"32-true\"` -\\> `\"precision\": \"bf16-true\"`\n",
    "\n",
    "Keep all other values the same as the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of the training time and memory, which is printed at the end of the training job.\n",
    "\n",
    "This job should run much faster, and use much less memory, but it may also have higher loss than the previous run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Mixed precision\n",
    "\n",
    "With mixed precision, we get back some of the lost precision in the results, at the cost of additional memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cfg`, change:\n",
    "\n",
    "-   `\"precision\": \"bf16-true\"` -\\> `\"precision\": \"bf16-mixed\"`\n",
    "\n",
    "Keep all other values the same as the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of the training time and memory, which is printed at the end of the training job.\n",
    "\n",
    "You may notice that this job is faster than the equivalent full precision job, with comparable loss after two epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Larger model\n",
    "\n",
    "We’ve gained so much GPU memory back with these techniques, we can even train a larger model. Let’s switch to a larger BLIP-2 model while keeping `bf16-true` precision:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cfg`, change:\n",
    "\n",
    "-   `\"model_name\": \"Salesforce/blip2-opt-2.7b\"` -\\> `\"Salesforce/blip2-opt-6.7b\"`\n",
    "-   `\"batch_size\": 16` -\\> `\"batch_size\": 32`\n",
    "-   `\"accumulate_grad_batches\": 4` -\\> `\"accumulate_grad_batches\": 2`\n",
    "-   `\"precision\": \"bf16-mixed\"` -\\> `\"bf16-true\"`\n",
    "\n",
    "Leave all other values the same as the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of the training time and memory, which is printed at the end of the training job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Even larger model\n",
    "\n",
    "Let us try an even larger model. We will change out the LLM from `opt-6.7b` (6.7B parameters) to `flan-t5-xxl` (about 11B parameters)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cfg`, change:\n",
    "\n",
    "-   `\"model_name\": \"Salesforce/blip2-opt-6.7b\"` -\\> `\"Salesforce/blip2-flan-t5-xxl\"`\n",
    "\n",
    "Leave all other values the same as the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run is expected to fail with OOM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Even larger model with even smaller batch size\n",
    "\n",
    "Even if we reduce to the smallest possible batch size, the `flan-t5-xxl` model is still too large for us to train on this GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cfg`, change:\n",
    "\n",
    "-   `\"batch_size\": 32` -\\> `\"batch_size\": 1`\n",
    "-   `\"accumulate_grad_batches\": 2` -\\> `\"accumulate_grad_batches\": 1`\n",
    "\n",
    "Leave all other values the same as the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This run will OOM, too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Optimizer without state (SGD)\n",
    "\n",
    "The previous XXL runs fail partly because optimizer state takes a lot of memory. Next, we keep the XXL model but switch to an optimizer with no state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cfg`, change:\n",
    "\n",
    "-   `\"batch_size\": 1` -\\> `\"batch_size\": 32`\n",
    "-   `\"accumulate_grad_batches\": 1` -\\> `\"accumulate_grad_batches\": 2`\n",
    "-   `\"optim\": \"adamw\"` -\\> `\"optim\": \"sgd\"`\n",
    "\n",
    "Leave all other values the same as the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of training time and memory. Compare `Optim` memory with the previous run.\n",
    "\n",
    "While we have freed up enough memory to train the XXL model, the result may not be great - the loss after 2 epochs may be poor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Low-memory optimizer alternative (`adam_8bit`)\n",
    "\n",
    "Another option is 8-bit Adam, which keeps optimizer state in reduced precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cfg`, change:\n",
    "\n",
    "-   `\"optim\": \"sgd\"` -\\> `\"optim\": \"adam_8bit\"`\n",
    "-   `\"batch_size\": 32` -\\> `\"batch_size\": 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep all other values the same as the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of training time and memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Activation checkpointing\n",
    "\n",
    "Another way to reduce memory usage (at the cost of compute) is with activation checkpointing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cfg`, change:\n",
    "\n",
    "-   `\"act_ckpt\": False` -\\> `\"act_ckpt\": True`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep all other values the same as the previous experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of training time and memory, especially the activation memory.\n",
    "\n",
    "We will see that we have saved on the activation memory, but at the cost of much slower training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: CPU offload with DeepSpeed\n",
    "\n",
    "Finally, another way to save memory at the cost of compute is to use DeepSpeed CPU offload to move optimizer state off the GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `cfg`, change:\n",
    "\n",
    "-   `\"model_name\": \"Salesforce/blip2-flan-t5-xxl\"`\n",
    "-   `\"precision\": \"bf16-true\"`\n",
    "-   `\"optim\": \"deepspeed_cpu\"`\n",
    "-   `\"act_ckpt\": False`\n",
    "-   `\"strategy\": \"deepspeed_stage_2_offload\"`\n",
    "-   `\"batch_size\": 32`\n",
    "-   `\"accumulate_grad_batches\": 2`\n",
    "-   `\"max_steps\": 2`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This resets us to the “Even larger model” settings and then adds CPU offload. “Stage 2” here refers to ZeRO stage 2 - offloading optimizer state and gradients.\n",
    "\n",
    "We set the maximum number of steps to 2 in this case, because training with CPU offload will be *so* slow - we really don’t want to let it run to the end of 2 epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a note of training time and memory. We should see lower GPU memory pressure, with extra overhead from CPU offload.\n",
    "\n",
    "In this run, the memory breakdown prints optimizer memory separately for GPU (`Optim GPU`) and CPU (`Optim CPU`). With DeepSpeed offload, a large fraction of optimizer state should move to CPU, so we expect `Optim CPU` to be high while `Optim GPU` stays much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment: Parameter efficient fine tuning\n",
    "\n",
    "If we are only fine-tuning, not training a model from scratch, we can also consider LoRA and QLoRA. Let’s try it first with our XXL model.\n",
    "\n",
    "We are going to use the `fine-tune-blip-lora.py` script for our PEFT experiments, so open that and note"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `cfg` in `fine-tune-blip-lora.py` to this LoRA configuration:\n",
    "\n",
    "``` python\n",
    "cfg = {\n",
    "    \"model_name\": \"Salesforce/blip2-flan-t5-xxl\",\n",
    "    \"lr\": 5e-6,\n",
    "    \"batch_size\": 32,\n",
    "    \"accumulate_grad_batches\": 2,\n",
    "    \"precision\": \"32-true\",\n",
    "    \"optim\": \"adamw\",\n",
    "    \"act_ckpt\": False,\n",
    "    \"max_epochs\": 2,\n",
    "    \"max_steps\": -1,\n",
    "    \"devices\": 1,\n",
    "    \"strategy\": \"auto\",\n",
    "    \"num_workers\": 8,\n",
    "    \"limit_train_batches\": 1.0,\n",
    "    \"enable_checkpointing\": False,\n",
    "    \"num_train_samples\": 512,\n",
    "    \"save_model\": False,\n",
    "    \"use_lora\": True,\n",
    "    \"use_qlora\": False,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference). Also check the `[cfg]` output near the top of the run to confirm the script is using the config you intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip-lora.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The memory required is much smaller! Earlier, we saw that full fine-tuning went OOM on this model in this configuration, but now we can fit it easily. Although the base model weights are still loaded (they are used in the forward/backward pass), there is only a small number of trainable parameters (just the adapter weights), so the gradients are much smaller and the optimizer state is much smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also further reduce the memory required by quantizing the base model weights:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `cfg` in `fine-tune-blip-lora.py` to this QLoRA configuration:\n",
    "\n",
    "``` python\n",
    "cfg = {\n",
    "    \"model_name\": \"Salesforce/blip2-flan-t5-xxl\",\n",
    "    \"lr\": 5e-6,\n",
    "    \"batch_size\": 64,\n",
    "    \"accumulate_grad_batches\": 1,\n",
    "    \"precision\": \"32-true\",\n",
    "    \"optim\": \"adamw\",\n",
    "    \"act_ckpt\": False,\n",
    "    \"max_epochs\": 2,\n",
    "    \"max_steps\": -1,\n",
    "    \"devices\": 1,\n",
    "    \"strategy\": \"auto\",\n",
    "    \"num_workers\": 8,\n",
    "    \"limit_train_batches\": 1.0,\n",
    "    \"enable_checkpointing\": False,\n",
    "    \"num_train_samples\": 512,\n",
    "    \"save_model\": False,\n",
    "    \"use_lora\": False,\n",
    "    \"use_qlora\": True,\n",
    "    \"lora_r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After updating the config and saving the script, run the following cell, and also watch the `nvtop` output as it runs (take a screenshot of `nvtop` for later reference). Also check the `[cfg]` output near the top of the run to confirm the script is using the config you intended."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# runs in the Jupyter service on node-llm-single\n",
    "python fine-tune-blip-lora.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have finished, download this notebook - which includes the output of each experiment stage - from the Jupyter environment for later reference."
   ]
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 4,
 "metadata": {
  "kernelspec": {
   "name": "bash",
   "display_name": "Bash",
   "language": "bash"
  },
  "language_info": {
   "name": "bash",
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "shell"
  }
 }
}
